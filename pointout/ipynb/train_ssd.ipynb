{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD\n",
    "\n",
    "In this notebook we are going to train an SSD model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:04:54.287177Z",
     "start_time": "2019-06-14T07:04:51.348089Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "First, we load the data we generated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:04:54.312177Z",
     "start_time": "2019-06-14T07:04:54.289586Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = './dataset/patches_256_100'\n",
    "\n",
    "df_t = pd.read_csv('{}/annotations_train.csv'.format(PATH))\n",
    "df_v = pd.read_csv('{}/annotations_eval.csv'.format(PATH))\n",
    "\n",
    "# convert string of bbs into list of bbs\n",
    "df_t.annotations = anns_str2int(df_t.annotations.values)\n",
    "df_v.annotations = anns_str2int(df_v.annotations.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:04:54.319544Z",
     "start_time": "2019-06-14T07:04:54.314270Z"
    }
   },
   "outputs": [],
   "source": [
    "# add path to image name for simplicity\n",
    "df_t.img_name = ['{}/{}'.format(PATH, img) for img in df_t.img_name.values]\n",
    "df_v.img_name = ['{}/{}'.format(PATH, img) for img in df_v.img_name.values]\n",
    "\n",
    "#df_t.sample(5)\n",
    "\n",
    "#df_t = df_t[:32]\n",
    "#df_v = df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:04:54.325593Z",
     "start_time": "2019-06-14T07:04:54.321504Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training patches: \", len(df_t))\n",
    "print(\"Validation patches: \", len(df_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Now we define our Dataset, which will define how images and labels are passed to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:04:54.343710Z",
     "start_time": "2019-06-14T07:04:54.327429Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images, annotations, transforms=None):\n",
    "        self.images = images\n",
    "        self.annotations = annotations\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        # open image\n",
    "        img = open_image(self.images[ix])\n",
    "        anns = self.annotations[ix]   \n",
    "        # split in boxes and labels\n",
    "        bboxes = [ann[0] for ann in anns]\n",
    "        labels = [ann[1] for ann in anns]\n",
    "        # apply transforms\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(**{'image': img, 'bboxes': bboxes, 'labels': labels})\n",
    "            img, bboxes, labels = augmented['image'], augmented['bboxes'], augmented['labels']  \n",
    "        # normalize bboxes\n",
    "        bboxes = [bb_norm_xyxy(bb, img.shape[:2]) for bb in bboxes]\n",
    "        # join annotations\n",
    "        anns = [(bb, label) for bb, label in zip(bboxes, labels)]\n",
    "        # return tensor image and label\n",
    "        return torch.from_numpy(img.transpose((2,0,1)).astype(np.float32)/255), anns\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        images, bbs, labels = [], [], [] # list for each image a tensor of shape O x 4, O x 1\n",
    "        for imgs, anns in batch:\n",
    "            images.append(imgs)\n",
    "            bbs.append(torch.FloatTensor([ann[0] for ann in anns]))\n",
    "            labels.append(torch.FloatTensor([ann[1] for ann in anns]))\n",
    "        return torch.stack(images), (bbs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:04:54.484648Z",
     "start_time": "2019-06-14T07:04:54.345479Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose, Resize, HorizontalFlip, VerticalFlip, Transpose, RandomRotate90, HueSaturationValue, RandomBrightness, GaussNoise\n",
    ")\n",
    "\n",
    "def get_aug(aug, min_area=0., min_visibility=0.):\n",
    "    return Compose(aug, bbox_params={'format': 'coco', 'min_area': min_area, 'min_visibility': min_visibility, 'label_fields': ['labels']})\n",
    "\n",
    "trans = get_aug([\n",
    "    HorizontalFlip(),\n",
    "    VerticalFlip(),\n",
    "    Transpose(),\n",
    "    RandomRotate90(),\n",
    "    HueSaturationValue(),\n",
    "    RandomBrightness(),\n",
    "    GaussNoise()\n",
    "])\n",
    "\n",
    "#trans=None\n",
    "\n",
    "dataset = {\n",
    "    'train': MyDataset(df_t.img_name.values, df_t.annotations.values, trans),\n",
    "    'val': MyDataset(df_v.img_name.values, df_v.annotations.values)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:05:28.106720Z",
     "start_time": "2019-06-14T07:05:27.287582Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# visualize random images\n",
    "ds = dataset['train']\n",
    "fig, axs = plt.subplots(3, 4, figsize=(15,10))\n",
    "for i, _ax in enumerate(axs):\n",
    "    for ix, ax in enumerate(_ax):\n",
    "        ix = 28#random.randint(0, len(ds)-1)\n",
    "        img, anns = ds[ix]\n",
    "        # bring back image from tensor\n",
    "        img = img.numpy().transpose((1, 2, 0))  \n",
    "        ax = show_image(img, ax=ax)\n",
    "        # unnorm box\n",
    "        for bb, label in anns:\n",
    "            box = bb_unnorm_xywh(bb, img.shape[:2])\n",
    "            draw_rect(ax, box, 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Here we define our model. We will use a pretrained Resnet34 as a backbone network and define only the last layers to adapt to our problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:16:29.693882Z",
     "start_time": "2019-06-14T07:16:29.576977Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# ssd model\n",
    "\n",
    "import torchvision\n",
    "\n",
    "def flatten_conv(x, k):\n",
    "    return x.view(x.size(0), x.size(1)//k, -1).transpose(1,2)\n",
    "\n",
    "class out_conv(nn.Module):\n",
    "    def __init__(self, c_in, k, n_classes):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.oconv1 = nn.Conv2d(c_in, k*4, 3, padding=1)\n",
    "        self.oconv2 = nn.Conv2d(c_in, k*n_classes, 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        return [\n",
    "            flatten_conv(self.oconv1(x), self.k),\n",
    "            flatten_conv(self.oconv2(x), self.k)\n",
    "        ]\n",
    "\n",
    "def conv(c_i, c_o, stride=2, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(c_i, c_o, 3, stride=stride, padding=padding), \n",
    "        nn.ReLU(),\n",
    "        nn.BatchNorm2d(c_o)\n",
    "    )\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # get pre-trained resnet34\n",
    "        self.model = torchvision.models.resnet34(pretrained=True)\n",
    "                \n",
    "        self.k = [1, 1, 1]\n",
    "        self.conv1 = conv(512, 256)\n",
    "        self.out1 = out_conv(256, self.k[0], n_classes)\n",
    "        self.out2 = out_conv(512, self.k[1], n_classes)\n",
    "        self.out3 = out_conv(256, self.k[2], n_classes)\n",
    "\n",
    "        self.anchors, self.grid_size = self.get_anchors()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # resnet backbone\n",
    "        x = self.model.maxpool(self.model.relu(self.model.bn1(self.model.conv1(x))))\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x) \n",
    "        x = self.model.layer3(x) \n",
    "        x1 = self.model.layer4(x) \n",
    "        # ssd head\n",
    "        x2 = self.conv1(x1) \n",
    "        o1l, o1c = self.out1(x)\n",
    "        o2l, o2c = self.out2(x1)\n",
    "        o3l, o3c = self.out3(x2) \n",
    "        \n",
    "        return [\n",
    "            torch.cat([o1l,o2l,o3l],dim=1), \n",
    "            torch.cat([o1c,o2c,o3c],dim=1)\n",
    "        ]\n",
    "\n",
    "    def get_anchors(self):\n",
    "        scales = [16, 8, 4]               \n",
    "        centers = [(0.5, 0.5)] \n",
    "        size_scales = [2**0]#, 2**(1/3), 2**(2/3)]\n",
    "        aspect_ratios = [(1., 1.)]#, (2., 1.), (1., 2.)]\n",
    "        sizes = [(s*a[0], s*a[1]) for s in size_scales for a in aspect_ratios]\n",
    "        _, anchors, grid_size = generate_anchors(scales, centers, sizes)\n",
    "        return anchors, grid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:16:35.874365Z",
     "start_time": "2019-06-14T07:16:30.057121Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# test net\n",
    "\n",
    "cats = ['airplane', 'bg']\n",
    "\n",
    "net = Net(n_classes = len(cats))\n",
    "\n",
    "test_input = torch.randn((16, 3, 256, 256))\n",
    "pred_bbs, pred_labs = net(test_input)\n",
    "\n",
    "print(pred_bbs.shape) # should output BATCH_SIZE x NUM_ANCHORS x 4\n",
    "print(pred_labs.shape) # should output BATCH_SIZE x NUM_ANCHORS x NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T09:38:45.820131Z",
     "start_time": "2019-06-07T09:38:45.811712Z"
    }
   },
   "source": [
    "We can visualize our anchors and which will match the ground truth during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:21:13.608890Z",
     "start_time": "2019-06-14T07:21:12.403970Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# anchors\n",
    "anchors = net.anchors\n",
    "fig, axs = plt.subplots(3, 5, figsize=(15,10))\n",
    "for i, _ax in enumerate(axs):\n",
    "    for ix, ax in enumerate(_ax):\n",
    "        ix = random.randint(0, len(ds)-1)\n",
    "        img, anns = ds[ix]\n",
    "        bboxes = torch.tensor([ann[0] for ann in anns]).float()\n",
    "        _labels = [ann[1] for ann in anns]\n",
    "            \n",
    "        img = img.numpy().transpose((1,2,0))\n",
    "        ax = show_image(img, ax=ax)\n",
    "        \n",
    "        if bboxes.shape[0] > 0:\n",
    "            \n",
    "            # compute IoU between gt and anchors\n",
    "            overlaps = torchvision.ops.box_iou(bboxes, anchors)\n",
    "\n",
    "            # keep best match and all above a threshold\n",
    "            gt_overlap, gt_idx = map_to_ground_truth(overlaps)\n",
    "            threshold = 0.5\n",
    "        \n",
    "            labels = []\n",
    "            for idx, iou in zip(gt_idx, gt_overlap):\n",
    "                if iou > threshold: labels.append(_labels) \n",
    "                else: labels.append(len(cats)-1)\n",
    "\n",
    "            # draw boxes\n",
    "            for bb in bboxes:\n",
    "                bb = bb_unnorm_xywh(bb, img.shape[:2])\n",
    "                draw_rect(ax, bb, 'green')\n",
    "            \n",
    "            # draw matching anchors\n",
    "            for j, a in enumerate(anchors):\n",
    "                if labels[j] is not len(cats)-1:\n",
    "                    bb = bb_unnorm_xywh(a, img.shape[:2])\n",
    "                    draw_rect(ax, bb, edgecolor=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In order to train the network we need to define a Dataloader from our dataset in order to feed the network with batches of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T11:42:43.660248Z",
     "start_time": "2019-06-12T11:42:43.657586Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = {\n",
    "    'train': DataLoader(dataset['train'], batch_size=32,  shuffle=True, num_workers=4, collate_fn=dataset['train'].collate_fn),\n",
    "    'val': DataLoader(dataset['val'], batch_size=32,  shuffle=False, num_workers=4, collate_fn=dataset['val'].collate_fn)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T11:42:44.499024Z",
     "start_time": "2019-06-12T11:42:43.661173Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs, anns = next(iter(dataloader['train']))\n",
    "bbs, labs = anns\n",
    "print(imgs.shape, len(bbs), len(labs))\n",
    "bbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the optimizer and loss function to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T11:42:44.805531Z",
     "start_time": "2019-06-12T11:42:44.504965Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if we can use GPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # should output cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T11:42:44.829780Z",
     "start_time": "2019-06-12T11:42:44.806956Z"
    },
    "code_folding": [
     28
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    return torch.eye(num_classes)[labels.data.cpu()]\n",
    "\n",
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, num_classes, device):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "    def forward(self, pred, targ):\n",
    "        t = one_hot_embedding(targ, self.num_classes)\n",
    "        t = torch.tensor(t[:,:-1].contiguous()).to(self.device)\n",
    "        x = pred[:,:-1]\n",
    "        w = self.get_weight(x,t)\n",
    "        return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)/self.num_classes\n",
    "    def get_weight(self,x,t): return None\n",
    "    \n",
    "class FocalLoss(BCE_Loss):\n",
    "    def get_weight(self,x,t):\n",
    "        alpha,gamma = 0.25,2.\n",
    "        p = x.sigmoid()\n",
    "        pt = p*t + (1-p)*(1-t)\n",
    "        w = alpha*t + (1-alpha)*(1-t)\n",
    "        return torch.tensor(w * (1-pt).pow(gamma))\n",
    "    \n",
    "class SSD_Loss(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, anchors, grid_size, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.loss_f = FocalLoss(num_classes, device)\n",
    "        self.anchors = anchors.to(device)\n",
    "        self.grid_size = grid_size.to(device)\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, preds, target):\n",
    "        # predicted bbs\n",
    "        pred_bbs, pred_cs = preds \n",
    "        # ground truth   \n",
    "        tar_bbs, c_t = target # B x O x 4, B x O\n",
    "        # for each image in batch\n",
    "        loc_loss, clas_loss = torch.tensor(0., requires_grad=False).to(device), torch.tensor(0., requires_grad=False).to(device)\n",
    "        for pred_bb, pred_c, tar_bb, tar_c in zip(pred_bbs, pred_cs, tar_bbs, c_t):\n",
    "            labels = torch.ones(len(self.anchors))*(self.num_classes-1)\n",
    "            if tar_bb.shape[0] is not 0: # some images may have no detections\n",
    "                tar_bb = tar_bb.to(device)\n",
    "                overlaps = torchvision.ops.box_iou(tar_bb, self.anchors)\n",
    "                gt_overlap, gt_idx = map_to_ground_truth(overlaps)\n",
    "                # ids of anchors to match\n",
    "                pos = gt_overlap > self.threshold\n",
    "                pos_idx = torch.nonzero(pos)[:,0]\n",
    "                # ids of targets to match\n",
    "                tar_idx = gt_idx[pos_idx]\n",
    "                pred_bb = actn_to_bb(pred_bb, self.anchors, self.grid_size)\n",
    "                _anchors = pred_bb[pos_idx]\n",
    "                tar_bb = tar_bb[tar_idx]\n",
    "                loc_loss += (_anchors - tar_bb).abs().mean()\n",
    "                labels[pos_idx] = tar_c[tar_idx]\n",
    "            labels = labels.long().to(device)            \n",
    "            clas_loss += self.loss_f(pred_c, labels)\n",
    "        return clas_loss + loc_loss, loc_loss, clas_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T11:42:46.243064Z",
     "start_time": "2019-06-12T11:42:44.830820Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=3e-4)\n",
    "criterion = SSD_Loss(len(cats), net.anchors, net.grid_size, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T11:42:46.248904Z",
     "start_time": "2019-06-12T11:42:46.244113Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def metric(preds, target, threshold=0.5):\n",
    "    \n",
    "    # activate predictions\n",
    "    pred_bbs, pred_cs = preds\n",
    "    pred_bbs = torch.stack([actn_to_bb(bb, net.anchors.to(device), net.grid_size.to(device)) for bb in pred_bbs])\n",
    "    pred_cs = torch.argmax(pred_cs, dim=2)\n",
    "    \n",
    "    # ground truth\n",
    "    tar_bbs, tar_cs = target\n",
    "    \n",
    "    # for each image in batch\n",
    "    f1 = []\n",
    "    for pred_bb, pred_c, tar_bb, tar_c in zip(pred_bbs, pred_cs, tar_bbs, tar_cs):\n",
    "        \n",
    "        # remove bg \n",
    "        ixs = (pred_c != len(cats) - 1).nonzero().view(-1)\n",
    "        pred_bb, pred_c = pred_bb[ixs], pred_c[ixs]\n",
    "        ixs = (tar_c != len(cats) - 1).nonzero().view(-1)\n",
    "        tar_bb, tar_c = tar_bb[ixs], tar_c[ixs]\n",
    "    \n",
    "        # compute F1\n",
    "        f1.append(F1(pred_bb, pred_c, tar_bb, tar_c))\n",
    "        \n",
    "    return np.array(f1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can proceed with the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T11:42:46.271834Z",
     "start_time": "2019-06-12T11:42:46.249918Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "    print('Training ...')\n",
    "    model.train()\n",
    "    losses, loc_losses, cls_losses = [], [], []\n",
    "    for imgs, anns in tqdm(dataloader, ascii=True):        \n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model(imgs)       \n",
    "        loss, loc_loss, cls_loss = criterion(outputs, anns)\n",
    "        losses.append(loss.item())\n",
    "        loc_losses.append(loc_loss.item())\n",
    "        cls_losses.append(cls_loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.mean(losses), np.mean(loc_losses), np.mean(cls_losses)\n",
    "\n",
    "def test(model, dataloader, criterion, metric):\n",
    "    print('Evaluating ...')    \n",
    "    model.eval()\n",
    "    losses, loc_losses, cls_losses, acc = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, anns in tqdm(dataloader, ascii=True):\n",
    "            imgs = imgs.to(device)       \n",
    "            outputs = model(imgs)        \n",
    "            loss, loc_loss, cls_loss = criterion(outputs, anns)\n",
    "            losses.append(loss.item())\n",
    "            loc_losses.append(loc_loss.item())\n",
    "            cls_losses.append(cls_loss.item())\n",
    "            acc.append(metric(outputs, anns))\n",
    "    return np.mean(losses), np.mean(loc_losses), np.mean(cls_losses), np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T11:59:36.776191Z",
     "start_time": "2019-06-12T11:42:46.272811Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# training\n",
    "net.to(device)\n",
    "EPOCHS = 200\n",
    "train_loss, train_loc_loss, train_cls_loss = [], [], []\n",
    "val_loss, val_loc_loss, val_cls_loss, acc, best_acc = [], [], [], [], 0\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print('Epoch: {}/{}'.format(epoch+1, EPOCHS))\n",
    "    \n",
    "    t_loss, t_loc_loss, t_cls_loss = train(net, dataloader['train'], criterion, optimizer)\n",
    "    train_loss.append(t_loss)\n",
    "    \n",
    "    v_loss, v_loc_loss, v_cls_loss, v_acc = test(net, dataloader['val'], criterion, metric)        \n",
    "    val_loss.append(v_loss)\n",
    "    acc.append(v_acc)\n",
    "    \n",
    "    print('Train Loss: {:.5f} {:.5f} {:.5f}. Val Loss: {:.5f} {:.5f} {:.5f}. Val acc: {:.5f}'.format(\n",
    "        t_loss, t_loc_loss, t_cls_loss, v_loss, v_loc_loss, v_cls_loss, v_acc)\n",
    "    )\n",
    "    \n",
    "    # keep best model\n",
    "    if v_acc > best_acc:\n",
    "        best_acc = v_acc\n",
    "        torch.save(net.state_dict(), './state_dict.pth')\n",
    "        print('Best acc {}, model saved'.format(best_acc))\n",
    "        \n",
    "print('Best acc {}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the training profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T07:29:26.249558Z",
     "start_time": "2019-06-14T07:29:25.805346Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 8))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "ax1.plot(train_loss, linewidth=3, label='train')\n",
    "ax1.plot(val_loss, ':', linewidth=3,  label='val')\n",
    "ax1.set_title(\"Loss\")\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid()\n",
    "ax2.plot(acc, linewidth=3, label=\"max: {:.4f}\".format(np.array(acc).max()))\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.grid()\n",
    "ax2.legend(loc='bottom right',handlelength=0, handletextpad=0, fancybox=True)\n",
    "ax2.set_xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Load the best model and make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T11:59:37.077854Z",
     "start_time": "2019-06-12T11:59:37.038518Z"
    }
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('state_dict.pth'))\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T12:00:11.175503Z",
     "start_time": "2019-06-12T12:00:10.558611Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# visualize random patches\n",
    "\n",
    "ds = dataset['val']\n",
    "fig, axs = plt.subplots(3, 4, figsize=(15,10))\n",
    "for i, _ax in enumerate(axs):\n",
    "    for ix, ax in enumerate(_ax):    \n",
    "        ix = random.randint(0, len(ds)-1)\n",
    "        img, anns = ds[ix]      \n",
    "        # plot image and bbs\n",
    "        _img = img.numpy().transpose((1,2,0))\n",
    "        ax = show_image(_img, ax=ax)\n",
    "        for bb, label in anns:\n",
    "            bb = bb_unnorm_xywh(bb, _img.shape[:2])\n",
    "            draw_rect(ax, bb, 'white')\n",
    "        # activate preds\n",
    "        preds = net(img.unsqueeze(0).to(device))\n",
    "        pred_bbs, pred_cs = preds\n",
    "        pred_c = torch.softmax(pred_cs[0], dim=1) \n",
    "        val, pred_c = torch.max(pred_c, 1)            \n",
    "        pred_bb = actn_to_bb(pred_bbs[0], net.anchors.to(device), net.grid_size.to(device))\n",
    "                    \n",
    "        bbs = torch.FloatTensor([ann[0] for ann in anns])\n",
    "        labels = torch.tensor([ann[1] for ann in anns])\n",
    "        ax.set_title('F1: {:.3f}'.format(F1(pred_bb, pred_c, bbs.to(device), labels)))\n",
    "\n",
    "        # plot preds\n",
    "        keep_ids = (pred_c < len(cats) - 1).nonzero()\n",
    "        if keep_ids.shape[0] > 0:\n",
    "            # remove bg\n",
    "            keep_ids = keep_ids[:,0]\n",
    "            pred_c = pred_c[keep_ids]\n",
    "            pred_bb = pred_bb[keep_ids]\n",
    "            # nms\n",
    "            nms_ixs = torchvision.ops.nms(pred_bb, pred_c.float(), iou_threshold=0.5)\n",
    "            pred_bb, pred_c = pred_bb[nms_ixs], pred_c[nms_ixs]\n",
    "            # compute F1\n",
    "            for bb in pred_bb:\n",
    "                bb = bb_unnorm_xywh(bb, img.shape[1:])\n",
    "                draw_rect(ax, bb, edgecolor=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T12:02:57.330231Z",
     "start_time": "2019-06-12T12:02:57.323690Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# detect on full image\n",
    "\n",
    "mosaics_v = pd.read_csv(\"{}/mosaics_eval.csv\".format(PATH))\n",
    "ix = 2#random.randint(0, len(mosaics_v)-1)\n",
    "window = 256\n",
    "size = 256\n",
    "stride = 100\n",
    "ratio = window / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T12:03:00.290380Z",
     "start_time": "2019-06-12T12:02:57.450276Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# plot image\n",
    "\n",
    "mosaics_v.annotations = anns_str2int(mosaics_v.annotations.values)\n",
    "mosaic = mosaics_v.loc[ix]\n",
    "img_ori = open_image(\"./dataset/{}\".format(mosaic.img_name))\n",
    "anns_ori = mosaic.annotations\n",
    "\n",
    "mosaic = mosaic.mosaic\n",
    "mosaic = mosaic.split(',')[:-1]\n",
    "mosaic = [m.split(' ')[:-1] for m in mosaic]\n",
    "shape = (len(mosaic), len(mosaic[0]))\n",
    "\n",
    "_bbs, _labs = [], []\n",
    "with torch.no_grad():    \n",
    "    #fig, axs = plt.subplots(2,2, figsize=(20,20))\n",
    "    fig, axs = plt.subplots(shape[0], shape[1], figsize=(20,20))\n",
    "    for i, _ax in enumerate(axs):\n",
    "        for j, ax in enumerate(_ax):\n",
    "            \n",
    "            ix = int(mosaic[i][j])\n",
    "            img, anns = dataset['val'][ix] # make sure is in same order\n",
    "            \n",
    "             # plot image and bbs\n",
    "            _img = img.numpy().transpose((1,2,0))\n",
    "            ax = show_image(_img, ax=ax)\n",
    "            for bb, label in anns:\n",
    "                bb = bb_unnorm_xywh(bb, _img.shape[:2])\n",
    "                draw_rect(ax, bb, 'white')\n",
    "            # activate preds\n",
    "            preds = net(img.unsqueeze(0).to(device))\n",
    "            pred_bbs, pred_cs = preds\n",
    "            pred_c = torch.softmax(pred_cs[0], dim=1) \n",
    "            val, pred_c = torch.max(pred_c, 1)            \n",
    "            pred_bb = actn_to_bb(pred_bbs[0], net.anchors.to(device), net.grid_size.to(device))\n",
    "\n",
    "            bbs = torch.FloatTensor([ann[0] for ann in anns])\n",
    "            labels = torch.tensor([ann[1] for ann in anns])\n",
    "            #ax.set_title('F1: {:.3f}'.format(F1(pred_bb, pred_c, bbs.to(device), labels)))\n",
    "\n",
    "            # plot preds\n",
    "            keep_ids = (pred_c < len(cats) - 1).nonzero()\n",
    "            if keep_ids.shape[0] > 0:\n",
    "                # remove bg\n",
    "                keep_ids = keep_ids[:,0]\n",
    "                pred_c = pred_c[keep_ids]\n",
    "                pred_bb = pred_bb[keep_ids]\n",
    "                # nms\n",
    "                nms_ixs = torchvision.ops.nms(pred_bb, pred_c.float(), iou_threshold=0.5)\n",
    "                pred_bb, pred_c = pred_bb[nms_ixs], pred_c[nms_ixs]\n",
    "                # compute F1\n",
    "                for bb in pred_bb:\n",
    "                    bb = bb_unnorm_xywh(bb, img.shape[1:])\n",
    "                    draw_rect(ax, bb, edgecolor=\"red\")\n",
    "                # keep preds\n",
    "                _bbs += [bb_unnorm_window_xyxy(ratio*bb.cpu(), img.shape[1:], img_ori.shape, j, i, shape[1], shape[0], window, stride) for bb in pred_bb]\n",
    "                _labs += pred_c.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T12:03:00.780719Z",
     "start_time": "2019-06-12T12:03:00.291528Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 2nd nms\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "ax = show_image(img_ori, ax = ax)\n",
    "\n",
    "bbs = torch.tensor(_bbs).to(device)\n",
    "labs = torch.tensor(_labs).float().to(device)\n",
    "nms_idx = torchvision.ops.nms(bbs, labs, 0.3)\n",
    "\n",
    "bbs = bbs[nms_idx]\n",
    "labs = labs[nms_idx]\n",
    "\n",
    "for bb, label in anns_ori:\n",
    "    if label == 0: \n",
    "        draw_rect(ax, bb)\n",
    "for bb in bbs:\n",
    "    bb = xyxy2xywh(bb.cpu().numpy())\n",
    "    draw_rect(ax, bb, edgecolor=\"red\")\n",
    "    \n",
    "bbs_ori = torch.FloatTensor([xywh2xyxy(ann[0]) for ann in anns_ori])\n",
    "labels_ori = torch.tensor([ann[1] for ann in anns_ori])\n",
    "ax.set_title('F1: {:.4f}'.format(F1(bbs, labs.long(), bbs_ori, labels_ori)), fontsize=30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

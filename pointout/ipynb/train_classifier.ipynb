{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "In this first step we are going to trian a classifier to say if there is an airplane or not in a patch ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:49.876693Z",
     "start_time": "2019-06-07T08:13:48.944971Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import matplotlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "First, we load the data we generated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:49.903297Z",
     "start_time": "2019-06-07T08:13:49.878460Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = './dataset/patches_256_200'\n",
    "\n",
    "df_t = pd.read_csv('{}/annotations_train.csv'.format(PATH))\n",
    "df_v = pd.read_csv('{}/annotations_eval.csv'.format(PATH))\n",
    "\n",
    "# convert string of bbs into list of bbs\n",
    "df_t.annotations = anns_str2int(df_t.annotations.values)\n",
    "df_v.annotations = anns_str2int(df_v.annotations.values)\n",
    "#df_t = df_t[:1]\n",
    "#df_v = df_t\n",
    "\n",
    "df_t.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column with a 1 if there are planes in the patch and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:49.924086Z",
     "start_time": "2019-06-07T08:13:49.905286Z"
    }
   },
   "outputs": [],
   "source": [
    "df_t[\"label\"] = [int(len(anns) > 0) for anns in df_t.annotations.values]\n",
    "df_v[\"label\"] = [int(len(anns) > 0) for anns in df_v.annotations.values]\n",
    "\n",
    "# add path to image name for simplicity\n",
    "df_t.img_name = ['{}/{}'.format(PATH, img) for img in df_t.img_name.values]\n",
    "df_v.img_name = ['{}/{}'.format(PATH, img) for img in df_v.img_name.values]\n",
    "\n",
    "\n",
    "df_t.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:17:48.655592Z",
     "start_time": "2019-06-07T08:17:48.644744Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training patches: \", len(df_t))\n",
    "print(\"Validation patches: \", len(df_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Now we define our Dataset, which will define how images and labels are passed to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:50.116643Z",
     "start_time": "2019-06-07T08:13:49.926395Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images, labels, transforms=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        # open image\n",
    "        img = open_image(self.images[ix])\n",
    "        label = self.labels[ix]\n",
    "        # apply transforms\n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=img)\n",
    "            img = augmented['image'] \n",
    "        # return tensor image and label\n",
    "        return torch.from_numpy(img.transpose((2,0,1)).astype(np.float32)/255), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:50.198175Z",
     "start_time": "2019-06-07T08:13:50.118142Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose, Resize, HorizontalFlip, VerticalFlip, Transpose, RandomRotate90, HueSaturationValue, RandomBrightness, GaussNoise\n",
    ")\n",
    "\n",
    "trans = {\n",
    "    'train': Compose([\n",
    "        Resize(224,224),\n",
    "        HorizontalFlip(),\n",
    "        VerticalFlip(),\n",
    "        Transpose(),\n",
    "        RandomRotate90(),\n",
    "        HueSaturationValue(),\n",
    "        RandomBrightness(),\n",
    "        GaussNoise()\n",
    "    ]),\n",
    "    'val': Resize(224, 224)\n",
    "}\n",
    "\n",
    "#trans = None\n",
    "\n",
    "dataset = {\n",
    "    'train': MyDataset(df_t.img_name.values, df_t.label.values, trans['train']),\n",
    "    'val': MyDataset(df_v.img_name.values, df_v.label.values, trans['val'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:51.064960Z",
     "start_time": "2019-06-07T08:13:50.199910Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# visualize random images\n",
    "\n",
    "ds = dataset['train']\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, figsize=(15,10))\n",
    "for i, _ax in enumerate(axs):\n",
    "    for ix, ax in enumerate(_ax):\n",
    "        \n",
    "        ix = random.randint(0, len(ds)-1)\n",
    "        img, label = ds[ix]\n",
    "\n",
    "        # bring back image from tensor\n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        \n",
    "        ax = show_image(img, ax=ax)\n",
    "        ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model\n",
    "\n",
    "Here we define our model. We will use a pretrained Resnet34 as a backbone network and define only the last layer to adapt to our problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:51.117719Z",
     "start_time": "2019-06-07T08:13:51.066522Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Net, self).__init__()\n",
    "        # get pre-trained resnet34\n",
    "        self.model = torchvision.models.resnet34()\n",
    "        # set new fc layer with our classes    \n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:51.157897Z",
     "start_time": "2019-06-07T08:13:51.120334Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check if we can use GPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device) # should output cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:53.570172Z",
     "start_time": "2019-06-07T08:13:51.160484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = Net()\n",
    "\n",
    "# copy net to GPU\n",
    "net.to(device)\n",
    "\n",
    "# test net\n",
    "test_input = torch.randn((16, 3, 224, 224))\n",
    "output = net(test_input.to(device))\n",
    "print(output.shape) # should output BATCH_SIZE x NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In order to train the network we need to define a Dataloader from our dataset in order to feed the network with batches of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:53.576120Z",
     "start_time": "2019-06-07T08:13:53.572242Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = {\n",
    "    'train': DataLoader(dataset['train'], batch_size=16,  shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(dataset['val'], batch_size=16,  shuffle=False, num_workers=4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:54.155335Z",
     "start_time": "2019-06-07T08:13:53.578278Z"
    }
   },
   "outputs": [],
   "source": [
    "imgs, labs = next(iter(dataloader['train']))\n",
    "print(imgs.shape, labs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the optimizer and loss function to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:54.163255Z",
     "start_time": "2019-06-07T08:13:54.158097Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:54.177281Z",
     "start_time": "2019-06-07T08:13:54.165571Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    _, preds = torch.max(preds.data, 1)\n",
    "    total = labels.size(0)\n",
    "    correct = (preds == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can proceed with the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:13:54.195001Z",
     "start_time": "2019-06-07T08:13:54.179260Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "    print('Training ...')\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for imgs, labels in tqdm(dataloader, ascii=True):        \n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)       \n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.mean(losses)\n",
    "\n",
    "def test(model, dataloader, criterion, metric):\n",
    "    print('Evaluating ...')    \n",
    "    model.eval()\n",
    "    losses, acc = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(dataloader, ascii=True):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)        \n",
    "            outputs = model(imgs)        \n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            acc.append(metric(outputs, labels))\n",
    "    return np.mean(losses), np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:15:27.028327Z",
     "start_time": "2019-06-07T08:13:54.197338Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "EPOCHS = 30\n",
    "train_loss = []\n",
    "val_loss, acc, best_acc = [], [], 0\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print('Epoch: {}/{}'.format(epoch+1, EPOCHS))\n",
    "    \n",
    "    t_loss = train(net, dataloader['train'], criterion, optimizer)\n",
    "    train_loss.append(t_loss)\n",
    "    \n",
    "    v_loss, v_acc = test(net, dataloader['val'], criterion, accuracy)        \n",
    "    val_loss.append(v_loss)\n",
    "    acc.append(v_acc)\n",
    "    \n",
    "    print('Train Loss: {:.5f}. Val Loss: {:.5f}. Val acc: {:.5f}'.format(t_loss, v_loss, v_acc))\n",
    "    \n",
    "    # keep best model\n",
    "    if v_acc > best_acc:\n",
    "        best_acc = v_acc\n",
    "        torch.save(net.state_dict(), './state_dict.pth')\n",
    "        print('Best acc {}, model saved'.format(best_acc))\n",
    "        \n",
    "print('Best acc {}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the training profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:15:27.378912Z",
     "start_time": "2019-06-07T08:15:27.030383Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 8))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "ax1.plot(train_loss, linewidth=3, label='train')\n",
    "ax1.plot(val_loss, ':', linewidth=3,  label='val')\n",
    "ax1.set_title(\"Loss\")\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid()\n",
    "ax2.plot(acc, linewidth=3, label=\"max: {:.4f}\".format(np.array(acc).max()))\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.grid()\n",
    "ax2.legend(loc='bottom right',handlelength=0, handletextpad=0, fancybox=True)\n",
    "ax2.set_xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Load the best model and make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:15:27.451104Z",
     "start_time": "2019-06-07T08:15:27.380502Z"
    }
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('state_dict.pth'))\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-07T08:15:28.359861Z",
     "start_time": "2019-06-07T08:15:27.452858Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize random images\n",
    "\n",
    "ds = dataset['val']\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, figsize=(15,10))\n",
    "for i, _ax in enumerate(axs):\n",
    "    for ix, ax in enumerate(_ax):\n",
    "        \n",
    "        ix = random.randint(0, len(ds)-1)\n",
    "        img, label = ds[ix]\n",
    "        \n",
    "        preds = net(img.unsqueeze(0).to(device)).squeeze()\n",
    "        pred_label = torch.argmax(preds, dim=0)\n",
    "\n",
    "        # bring back image from tensor\n",
    "        img = img.numpy().transpose((1, 2, 0))\n",
    "        \n",
    "        ax = show_image(img, ax=ax)\n",
    "        ax.set_title(\"GT: {} / Pred: {}\".format(label, pred_label), color = \"green\" if label == pred_label else \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
